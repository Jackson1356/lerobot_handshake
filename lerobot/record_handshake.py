"""
Records a handshake interaction dataset. Actions for the robot are generated by teleoperation 
with a leader arm to reach and shake hands with a person when they are ready.

The script uses handshake detection to automatically start recording when a person 
extends their hand for a handshake gesture.

Example:

```shell
python -m lerobot.record_handshake \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM1 \
    --robot.cameras="{ front: {type: opencv, index_or_path: /dev/video1, width: 640, height: 480, fps: 30}}" \
    --robot.id=my_follower_arm \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM0 \
    --teleop.id=my_leader_arm \
    --dataset.repo_id=your_username/handshake_dataset \
    --dataset.num_episodes=10 \
    --dataset.single_task="Shake hands with person when they extend their hand" \
    --dataset.episode_time_s=30 \      # 30-second recording episodes \
    --dataset.reset_time_s=15 \         # 15-second reset time between episodes \
    --display_data=true
```
You can use left arrow key to re-record the episode and right arrow key to early exit the recording.
"""


import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from pprint import pformat

import cv2
import numpy as np
import rerun as rr

from lerobot.common.cameras import (  # noqa: F401
    CameraConfig,  # noqa: F401
)
from lerobot.common.cameras.opencv.configuration_opencv import OpenCVCameraConfig  # noqa: F401
from lerobot.common.cameras.realsense.configuration_realsense import RealSenseCameraConfig  # noqa: F401
from lerobot.common.datasets.image_writer import safe_stop_image_writer
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from lerobot.common.datasets.utils import build_dataset_frame, hw_to_dataset_features
from lerobot.common.handshake_detection import ImprovedHandshakeDetector
from lerobot.common.policies.factory import make_policy
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.robots import (  # noqa: F401
    Robot,
    RobotConfig,
    make_robot_from_config,
    so101_follower,
)
from lerobot.common.teleoperators import (  # noqa: F401
    Teleoperator,
    TeleoperatorConfig,
    make_teleoperator_from_config,
    so101_leader,
)
from lerobot.common.utils.control_utils import (
    init_keyboard_listener,
    is_headless,
    predict_action,
    sanity_check_dataset_name,
    sanity_check_dataset_robot_compatibility,
)
from lerobot.common.utils.robot_utils import busy_wait
from lerobot.common.utils.utils import (
    get_safe_torch_device,
    init_logging,
    log_say,
)
from lerobot.common.utils.visualization_utils import _init_rerun
from lerobot.configs import parser
from lerobot.configs.policies import PreTrainedConfig

@dataclass
class HandshakeDatasetRecordConfig:
    repo_id: str
    single_task: str
    root: str | Path | None = None
    fps: int = 30
    episode_time_s: int | float = 30
    reset_time_s: int | float = 10
    num_episodes: int = 50
    video: bool = True

    handshake_confidence_threshold: float = 0.8
    handshake_detection_delay: float = 1.0
    handshake_timeout_s: float = 10.0

    num_image_writer_processes: int = 0
    num_image_writer_threads_per_camera: int = 4

    def __post_init__(self):
        if self.single_task is None:
            raise ValueError("You need to provide a task as argument in `single_task`.")


@dataclass
class HandshakeRecordConfig:
    robot: RobotConfig
    dataset: HandshakeDatasetRecordConfig
    teleop: TeleoperatorConfig | None = None
    policy: PreTrainedConfig | None = None
    display_data: bool = False
    play_sounds: bool = True
    resume: bool = False

    def __post_init__(self):
        # HACK: We parse again the cli args here to get the pretrained path if there was one.
        policy_path = parser.get_path_arg("policy")
        if policy_path:
            cli_overrides = parser.get_cli_overrides("policy")
            self.policy = PreTrainedConfig.from_pretrained(policy_path, cli_overrides=cli_overrides)
            self.policy.pretrained_path = policy_path

        if self.teleop is None and self.policy is None:
            raise ValueError("Choose a policy, a teleoperator or both to control the robot")

    @classmethod
    def __get_path_fields__(cls) -> list[str]:
        """This enables the parser to load config from the policy using `--policy.path=local/dir`"""
        return ["policy"]


def wait_for_handshake_detection(
    robot: Robot,
    handshake_detector: ImprovedHandshakeDetector,
    camera_name: str,
    timeout_s: float,
    confidence_threshold: float,
    detection_delay: float,
    display_data: bool = False,
    teleop: Teleoperator | None = None,
    episode_number: int = 0,
) -> bool:
    """
    Wait for handshake detection from the person.
    
    Returns:
        True if handshake detected, False if timeout
    """
    start_time = time.perf_counter()
    detection_start_time = None
    last_status_update = 0
    
    log_say("Waiting for person to extend their hand for handshake...", True)
    
    while time.perf_counter() - start_time < timeout_s:
        try:
            observation = robot.get_observation()
                
            frame = observation[camera_name]
            
            # Process teleop commands during waiting (so user can position robot)
            if teleop is not None:
                action = teleop.get_action()
                robot.send_action(action)
            
            # Detect handshake gesture
            detection_result = handshake_detector.detect_handshake_gesture(frame, visualize=True)
            
            if detection_result['ready'] and detection_result['confidence'] >= confidence_threshold:
                if detection_start_time is None:
                    detection_start_time = time.perf_counter()
                    log_say(f"Handshake detected! Waiting {detection_delay} seconds before starting recording...", True)
                
                # Wait for the specified delay after detection
                if time.perf_counter() - detection_start_time >= detection_delay:
                    log_say("Starting handshake recording now!", True)
                    return True
            else:
                # Reset detection timer if gesture is lost
                detection_start_time = None
            
            # Always display clean data to Rerun during waiting phase (no OpenCV window)
            # Only show essential robot arm states (6 joint values) + camera with pose detection
            
            # Update status every second
            current_time = time.perf_counter()
            if current_time - last_status_update >= 1.0:
                status_text = f"WAITING | Episode: {episode_number} | Time remaining: {timeout_s - (current_time - start_time):.1f}s"
                rr.log("status", rr.TextLog(status_text, level=rr.TextLogLevel.INFO))
                last_status_update = current_time
            
            # Robot joint positions (6 values only) - grouped in single chart
            annotated_frame = detection_result.get('annotated_frame')
            robot_joints = {}
            for obs, val in observation.items():
                if isinstance(val, float) and obs.endswith('.pos'):
                    # Collect all robot joint positions for single chart
                    robot_joints[obs] = val
                elif isinstance(val, np.ndarray):
                    if obs == camera_name and annotated_frame is not None:
                        # Camera with pose detection - consistent path
                        rr.log("camera_with_pose", rr.Image(annotated_frame), static=True)
                        # Raw camera - consistent path
                        rr.log("camera_raw", rr.Image(val), static=True)
                    else:
                        # Raw camera for other cameras
                        rr.log("camera_raw", rr.Image(val), static=True)
            
            # Log all robot joints as single chart
            if robot_joints:
                # Log each joint individually to create clean grouped chart
                for joint_name, joint_val in robot_joints.items():
                    rr.log(f"robot_joints/{joint_name}", rr.Scalar(joint_val))
            
            time.sleep(0.1)  # Small delay to prevent excessive CPU usage
            
        except KeyboardInterrupt:
            logging.info("KeyboardInterrupt received in handshake detection")
            break
        except Exception as e:
            logging.error(f"Error in handshake detection loop: {e}")
            time.sleep(0.1)
            continue
    
    log_say("Handshake detection timeout. Skipping this episode.", True)
    return False


@safe_stop_image_writer
def record_handshake_loop(
    robot: Robot,
    events: dict,
    fps: int,
    handshake_detector: ImprovedHandshakeDetector,
    main_camera_name: str,
    dataset: LeRobotDataset | None = None,
    teleop: Teleoperator | None = None,
    policy: PreTrainedPolicy | None = None,
    control_time_s: int | None = None,
    single_task: str | None = None,
    display_data: bool = False,
    episode_number: int = 0,
):
    if dataset is not None and dataset.fps != fps:
        raise ValueError(f"The dataset fps should be equal to requested fps ({dataset.fps} != {fps}).")

    # if policy is given it needs cleaning up
    if policy is not None:
        policy.reset()

    timestamp = 0
    start_episode_t = time.perf_counter()
    last_status_update = 0
    
    # FPS debugging
    frame_count = 0
    fps_debug_start = time.perf_counter()
    total_handshake_time = 0
    total_robot_time = 0
    total_busy_wait_time = 0
    total_teleop_time = 0
    total_dataset_time = 0
    total_display_time = 0
    total_build_frame_time = 0
    
    # Handshake detection fps control (run at lower rate to improve performance)
    handshake_fps = 10  # Run handshake detection at 10fps instead of 30fps
    last_handshake_time = 0
    cached_handshake_result = None
    cached_annotated_frame = None
    
    # Performance debugging: temporarily disable display to test bottleneck
    debug_disable_display = False  # Set to True to test if display is the bottleneck
    
    logging.info(f"Starting recording loop: target_fps={fps}, expected_duration={control_time_s}s")
    logging.info(f"Handshake detection will run at {handshake_fps}fps for better performance")
    if debug_disable_display:
        logging.info("DEBUG: Display operations disabled for performance testing")
    
    while timestamp < control_time_s:
        start_loop_t = time.perf_counter()

        if events["exit_early"]:
            events["exit_early"] = False
            break

        # Measure robot observation time
        robot_start = time.perf_counter()
        observation = robot.get_observation()
        robot_time = time.perf_counter() - robot_start
        total_robot_time += robot_time

        # Run handshake detection at reduced fps for better performance
        handshake_start = time.perf_counter()
        handshake_result = None
        current_time = time.perf_counter()
        
        if main_camera_name in observation:
            # Only run handshake detection every 1/handshake_fps seconds
            if current_time - last_handshake_time >= (1.0 / handshake_fps):
                frame = observation[main_camera_name]
                # Run with visualization only if display is enabled and not disabled for debugging
                should_visualize = display_data and not debug_disable_display
                full_result = handshake_detector.detect_handshake_gesture(frame, visualize=should_visualize)
                cached_handshake_result = full_result
                if should_visualize and 'annotated_frame' in full_result:
                    cached_annotated_frame = full_result['annotated_frame']
                else:
                    cached_annotated_frame = None
                last_handshake_time = current_time
            
            # Use cached result for dataset recording
            handshake_result = cached_handshake_result
            
            if handshake_result is not None:
                # Add handshake detection data to observation for dataset recording
                # (but filter out from Rerun display to keep it clean)
                handshake_ready = float(handshake_result['ready'])
                handshake_confidence = handshake_result['confidence']
                if handshake_result['hand_position'] is not None:
                    hand_position_x = float(handshake_result['hand_position'][0])
                    hand_position_y = float(handshake_result['hand_position'][1])
                else:
                    hand_position_x = -1.0
                    hand_position_y = -1.0
                
                # Add individual handshake values to observation (required by build_dataset_frame)
                observation["handshake_ready"] = handshake_ready
                observation["handshake_confidence"] = handshake_confidence
                observation["hand_position_x"] = hand_position_x
                observation["hand_position_y"] = hand_position_y
            else:
                # Fallback values if no handshake detection result yet
                observation["handshake_ready"] = 0.0
                observation["handshake_confidence"] = 0.0
                observation["hand_position_x"] = -1.0
                observation["hand_position_y"] = -1.0
        
        handshake_time = time.perf_counter() - handshake_start
        total_handshake_time += handshake_time

        # Build observation frame
        build_frame_start = time.perf_counter()
        if policy is not None or dataset is not None:
            observation_frame = build_dataset_frame(dataset.features, observation, prefix="observation")
        build_frame_time = time.perf_counter() - build_frame_start
        total_build_frame_time += build_frame_time

        # Get teleop action
        teleop_start = time.perf_counter()
        if policy is not None:
            action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
            action = {key: action_values[i].item() for i, key in enumerate(robot.action_features)}
        elif policy is None and teleop is not None:
            action = teleop.get_action()
        else:
            logging.info(
                "No policy or teleoperator provided, skipping action generation."
                "This is likely to happen when resetting the environment without a teleop device."
                "The robot won't be at its rest position at the start of the next episode."
            )
            continue
        teleop_time = time.perf_counter() - teleop_start
        total_teleop_time += teleop_time

        # Action can eventually be clipped using `max_relative_target`,
        # so action actually sent is saved in the dataset.
        sent_action = robot.send_action(action)

        # Dataset operations
        dataset_start = time.perf_counter()
        if dataset is not None:
            action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
            frame = {**observation_frame, **action_frame}
            dataset.add_frame(frame, task=single_task)
        dataset_time = time.perf_counter() - dataset_start
        total_dataset_time += dataset_time

        # Display operations
        display_start = time.perf_counter()
        if display_data and not debug_disable_display:
            # Use cached annotated frame from handshake detection
            annotated_frame = cached_annotated_frame
            
            # Update status every second 
            current_time = time.perf_counter()
            if current_time - last_status_update >= 1.0:
                actual_fps = frame_count / (current_time - fps_debug_start) if current_time > fps_debug_start else 0
                status_text = f"RECORDING | Episode: {episode_number} | Elapsed: {timestamp:.1f}s | Remaining: {max(0, control_time_s - timestamp):.1f}s | Actual FPS: {actual_fps:.1f}"
                rr.log("status", rr.TextLog(status_text, level=rr.TextLogLevel.INFO))
                logging.info(f"Recording progress: {actual_fps:.1f} actual fps vs {fps} target fps")
                last_status_update = current_time
            
            # Robot joint positions (6 values only) - grouped in single chart
            robot_joints = {}
            for obs, val in observation.items():
                if isinstance(val, float) and obs.endswith('.pos'):
                    # Collect all robot joint positions for single chart
                    robot_joints[obs] = val
                elif isinstance(val, np.ndarray):
                    if obs == main_camera_name and annotated_frame is not None:
                        # Camera with pose detection - consistent path
                        rr.log("camera_with_pose", rr.Image(annotated_frame), static=True)
                        # Raw camera - consistent path
                        rr.log("camera_raw", rr.Image(val), static=True)
                    else:
                        # Raw camera for other cameras
                        rr.log("camera_raw", rr.Image(val), static=True)
            
            # Log all robot joints as single chart
            if robot_joints:
                # Log each joint individually to create clean grouped chart
                for joint_name, joint_val in robot_joints.items():
                    rr.log(f"robot_joints/{joint_name}", rr.Scalar(joint_val))
            
            # Actions are sent to robot but NOT displayed in charts to keep it clean (6 values only)

        display_time = time.perf_counter() - display_start
        total_display_time += display_time

        dt_s = time.perf_counter() - start_loop_t
        busy_wait_start = time.perf_counter()
        busy_wait(1 / fps - dt_s)
        busy_wait_time = time.perf_counter() - busy_wait_start
        total_busy_wait_time += busy_wait_time

        timestamp = time.perf_counter() - start_episode_t
        frame_count += 1

    # Final fps analysis
    total_time = time.perf_counter() - fps_debug_start
    actual_fps = frame_count / total_time
    avg_handshake_time = total_handshake_time / frame_count if frame_count > 0 else 0
    avg_robot_time = total_robot_time / frame_count if frame_count > 0 else 0
    avg_busy_wait_time = total_busy_wait_time / frame_count if frame_count > 0 else 0
    avg_teleop_time = total_teleop_time / frame_count if frame_count > 0 else 0
    avg_dataset_time = total_dataset_time / frame_count if frame_count > 0 else 0
    avg_display_time = total_display_time / frame_count if frame_count > 0 else 0
    avg_build_frame_time = total_build_frame_time / frame_count if frame_count > 0 else 0
    
    logging.info(f"=== RECORDING PERFORMANCE DEBUG ===")
    logging.info(f"Target FPS: {fps}")
    logging.info(f"Actual FPS: {actual_fps:.2f}")
    logging.info(f"Handshake detection FPS: {handshake_fps} (optimized)")
    logging.info(f"Frame count: {frame_count}")
    logging.info(f"Total time: {total_time:.2f}s")
    logging.info(f"Expected frames at {fps}fps: {fps * control_time_s}")
    logging.info(f"")
    logging.info(f"=== TIMING BREAKDOWN (per frame) ===")
    logging.info(f"Robot observation:     {avg_robot_time*1000:.1f}ms")
    logging.info(f"Handshake detection:   {avg_handshake_time*1000:.1f}ms")
    logging.info(f"Build dataset frame:   {avg_build_frame_time*1000:.1f}ms")
    logging.info(f"Teleop/policy:         {avg_teleop_time*1000:.1f}ms")
    logging.info(f"Dataset operations:    {avg_dataset_time*1000:.1f}ms")
    logging.info(f"Display/Rerun:         {avg_display_time*1000:.1f}ms")
    logging.info(f"Busy wait:             {avg_busy_wait_time*1000:.1f}ms")
    total_accounted = (avg_robot_time + avg_handshake_time + avg_build_frame_time + 
                      avg_teleop_time + avg_dataset_time + avg_display_time + avg_busy_wait_time) * 1000
    logging.info(f"Total accounted:       {total_accounted:.1f}ms")
    logging.info(f"Target frame time:     {1000/fps:.1f}ms")
    logging.info(f"Performance ratio: {actual_fps/fps:.2f}")
    logging.info("======================================")


@parser.wrap()
def record_handshake(cfg: HandshakeRecordConfig) -> LeRobotDataset:
    init_logging()
    logging.info(pformat(asdict(cfg)))
    
    # Always initialize Rerun for live monitoring (robot states + pose detection)
    _init_rerun(session_name="handshake_recording")

    robot = make_robot_from_config(cfg.robot)
    teleop = make_teleoperator_from_config(cfg.teleop) if cfg.teleop is not None else None

    robot.connect()
    if teleop is not None:
        teleop.connect()

    # Debug camera fps configuration after connection
    if hasattr(robot, 'cameras') and robot.cameras:
        for camera_name, camera in robot.cameras.items():
            if hasattr(camera, 'fps'):
                logging.info(f"Camera '{camera_name}' configured fps: {camera.fps}")
            if hasattr(camera, 'videocapture') and camera.videocapture is not None:
                actual_camera_fps = camera.videocapture.get(cv2.CAP_PROP_FPS)
                logging.info(f"Camera '{camera_name}' actual hardware fps: {actual_camera_fps}")
    logging.info(f"Dataset recording fps: {cfg.dataset.fps}")

    # Initialize handshake detector
    try:
        handshake_detector = ImprovedHandshakeDetector(
            confidence_threshold=cfg.dataset.handshake_confidence_threshold
        )
        logging.info(f"Handshake detector initialized with confidence threshold: {cfg.dataset.handshake_confidence_threshold}")
    except ImportError as e:
        logging.error(f"Failed to initialize handshake detector: {e}")
        logging.error("Please install required dependencies: pip install mediapipe opencv-python")
        raise

    # Determine main camera name (assume first camera is the main one)
    if not hasattr(robot.config, 'cameras') or not robot.config.cameras:
        raise ValueError("Robot must have at least one camera configured for handshake detection")
    
    main_camera_name = list(robot.config.cameras.keys())[0]
    logging.info(f"Using camera '{main_camera_name}' for handshake detection")

    # Build dataset features (without handshake features to avoid build_dataset_frame issues)
    action_features = hw_to_dataset_features(robot.action_features, "action", cfg.dataset.video)
    obs_features = hw_to_dataset_features(robot.observation_features, "observation", cfg.dataset.video)
    
    # Add handshake detection features as a single grouped feature (like robot joints)
    handshake_features = {
        "observation.handshake": {
            "dtype": "float32",
            "shape": (4,),
            "names": ["handshake_ready", "handshake_confidence", "hand_position_x", "hand_position_y"],
        },
    }
    
    dataset_features = {**action_features, **obs_features, **handshake_features}
    

    if cfg.resume:
        dataset = LeRobotDataset(
            cfg.dataset.repo_id,
            root=cfg.dataset.root,
        )

        if hasattr(robot, "cameras") and len(robot.cameras) > 0:
            dataset.start_image_writer(
                num_processes=cfg.dataset.num_image_writer_processes,
                num_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            )
        sanity_check_dataset_robot_compatibility(dataset, robot, cfg.dataset.fps, dataset_features)
    else:
        # Create empty dataset or load existing saved episodes
        sanity_check_dataset_name(cfg.dataset.repo_id, cfg.policy)
        dataset = LeRobotDataset.create(
            cfg.dataset.repo_id,
            cfg.dataset.fps,
            root=cfg.dataset.root,
            robot_type=robot.name,
            features=dataset_features,
            use_videos=cfg.dataset.video,
            image_writer_processes=cfg.dataset.num_image_writer_processes,
            image_writer_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
        )

    # Load pretrained policy
    policy = None if cfg.policy is None else make_policy(cfg.policy, ds_meta=dataset.meta)

    # Status will be shown as text overlay - no separate charts needed

    listener, events = init_keyboard_listener()

    recorded_episodes = 0
    while recorded_episodes < cfg.dataset.num_episodes and not events["stop_recording"]:
        log_say(f"Preparing to record handshake episode {dataset.num_episodes}", cfg.play_sounds)
        
        # Wait for handshake detection before starting episode
        handshake_detected = wait_for_handshake_detection(
            robot=robot,
            handshake_detector=handshake_detector,
            camera_name=main_camera_name,
            timeout_s=cfg.dataset.handshake_timeout_s,
            confidence_threshold=cfg.dataset.handshake_confidence_threshold,
            detection_delay=cfg.dataset.handshake_detection_delay,
            display_data=cfg.display_data,
            teleop=teleop,  # Enable teleop during waiting phase
            episode_number=dataset.num_episodes,
        )
        
        if not handshake_detected:
            log_say("Skipping episode due to handshake detection timeout", cfg.play_sounds)
            continue
        
        log_say(f"Recording handshake episode {dataset.num_episodes}", cfg.play_sounds)
        record_handshake_loop(
            robot=robot,
            events=events,
            fps=cfg.dataset.fps,
            handshake_detector=handshake_detector,
            main_camera_name=main_camera_name,
            teleop=teleop,
            policy=policy,
            dataset=dataset,
            control_time_s=cfg.dataset.episode_time_s,
            single_task=cfg.dataset.single_task,
            display_data=cfg.display_data,
            episode_number=dataset.num_episodes,
        )

        # Execute a few seconds without recording to give time to manually reset the environment
        # Skip reset for the last episode to be recorded
        if not events["stop_recording"] and (
            (recorded_episodes < cfg.dataset.num_episodes - 1) or events["rerecord_episode"]
        ):
            log_say("Reset the environment for next handshake", cfg.play_sounds)
            
            # Reset phase - status will be visible in console logs
            
            # Use regular record loop for reset (without handshake detection and without display)
            from lerobot.record import record_loop
            record_loop(
                robot=robot,
                events=events,
                fps=cfg.dataset.fps,
                teleop=teleop,
                control_time_s=cfg.dataset.reset_time_s,
                single_task=cfg.dataset.single_task,
                display_data=False,  # Disable display during reset to avoid extra windows
            )

        if events["rerecord_episode"]:
            log_say("Re-record episode", cfg.play_sounds)
            events["rerecord_episode"] = False
            events["exit_early"] = False
            dataset.clear_episode_buffer()
            continue

        dataset.save_episode()
        recorded_episodes += 1

    log_say("Stop recording handshake dataset", cfg.play_sounds, blocking=True)

    robot.disconnect()
    if teleop is not None:
        teleop.disconnect()

    if not is_headless() and listener is not None:
        listener.stop()

    # Close any OpenCV resources (cleanup)
    cv2.destroyAllWindows()

    log_say("Exiting", cfg.play_sounds)
    return dataset


if __name__ == "__main__":
    record_handshake()












