"""
Records a handshake interaction dataset. Actions for the robot are generated by teleoperation 
with a leader arm to reach and shake hands with a person when they are ready.

The script uses handshake detection to automatically start recording when a person 
extends their hand for a handshake gesture.

Example:

```shell
python -m lerobot.record_handshake \
    --robot.type=so101_follower \
    --robot.port=/dev/ttyACM1 \
    --robot.cameras="{ front: {type: opencv, index_or_path: /dev/video1, width: 640, height: 480, fps: 30}}" \
    --robot.id=my_follower_arm \
    --teleop.type=so101_leader \
    --teleop.port=/dev/ttyACM0 \
    --teleop.id=my_leader_arm \
    --dataset.root=./data/folder_name/handshake_dataset \
    --dataset.num_episodes=10 \
    --dataset.single_task="Shake hands with person when they extend their hand" \
    --dataset.episode_time_s=30 \      # 30-second recording episodes \
    --dataset.reset_time_s=15 \         # 15-second reset time between episodes \
    --display_data=true
```
You can use left arrow key to re-record the episode and right arrow key to early exit the recording.
"""


import logging
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from pprint import pformat

import cv2
import numpy as np
import rerun as rr

from lerobot.common.cameras import (  # noqa: F401
    CameraConfig,  # noqa: F401
)
from lerobot.common.cameras.opencv.configuration_opencv import OpenCVCameraConfig  # noqa: F401
from lerobot.common.cameras.realsense.configuration_realsense import RealSenseCameraConfig  # noqa: F401
from lerobot.common.datasets.image_writer import safe_stop_image_writer
from lerobot.common.datasets.lerobot_dataset import LeRobotDataset
from lerobot.common.datasets.utils import build_dataset_frame, hw_to_dataset_features
from lerobot.common.handshake_detection import ImprovedHandshakeDetector
from lerobot.common.policies.factory import make_policy
from lerobot.common.policies.pretrained import PreTrainedPolicy
from lerobot.common.robots import (  # noqa: F401
    Robot,
    RobotConfig,
    make_robot_from_config,
    so101_follower,
)
from lerobot.common.teleoperators import (  # noqa: F401
    Teleoperator,
    TeleoperatorConfig,
    make_teleoperator_from_config,
    so101_leader,
)
from lerobot.common.utils.control_utils import (
    init_keyboard_listener,
    is_headless,
    predict_action,
    sanity_check_dataset_name,
    sanity_check_dataset_robot_compatibility,
)
from lerobot.common.utils.robot_utils import busy_wait
from lerobot.common.utils.utils import (
    get_safe_torch_device,
    init_logging,
    log_say,
)
from lerobot.common.utils.visualization_utils import _init_rerun
from lerobot.configs import parser
from lerobot.configs.policies import PreTrainedConfig

@dataclass
class HandshakeDatasetRecordConfig:
    repo_id: str | None = None
    single_task: str
    root: str | Path | None = None
    fps: int = 20
    episode_time_s: int | float = 30
    reset_time_s: int | float = 10
    num_episodes: int = 50
    video: bool = True

    handshake_confidence_threshold: float = 0.8
    handshake_detection_delay: float = 1.0
    handshake_timeout_s: float = 10.0
    handshake_detection_fps: int = 10  # Run handshake detection at lower FPS during recording

    num_image_writer_processes: int = 0
    num_image_writer_threads_per_camera: int = 4

    def __post_init__(self):
        if self.single_task is None:
            raise ValueError("You need to provide a task as argument in `single_task`.")
        # For local datasets, repo_id is optional but root is required
        if self.repo_id is None and self.root is None:
            raise ValueError("Either repo_id (for HuggingFace datasets) or root (for local datasets) must be specified")


@dataclass
class HandshakeRecordConfig:
    robot: RobotConfig
    dataset: HandshakeDatasetRecordConfig
    teleop: TeleoperatorConfig | None = None
    policy: PreTrainedConfig | None = None
    display_data: bool = False
    play_sounds: bool = True
    resume: bool = False

    def __post_init__(self):
        # HACK: We parse again the cli args here to get the pretrained path if there was one.
        policy_path = parser.get_path_arg("policy")
        if policy_path:
            cli_overrides = parser.get_cli_overrides("policy")
            self.policy = PreTrainedConfig.from_pretrained(policy_path, cli_overrides=cli_overrides)
            self.policy.pretrained_path = policy_path

        if self.teleop is None and self.policy is None:
            raise ValueError("Choose a policy, a teleoperator or both to control the robot")

    @classmethod
    def __get_path_fields__(cls) -> list[str]:
        """This enables the parser to load config from the policy using `--policy.path=local/dir`"""
        return ["policy"]


def wait_for_handshake_detection(
    robot: Robot,
    handshake_detector: ImprovedHandshakeDetector,
    camera_name: str,
    timeout_s: float,
    confidence_threshold: float,
    detection_delay: float,
    display_data: bool = False,
    teleop: Teleoperator | None = None,
    episode_number: int = 0,
) -> bool:
    """
    Wait for handshake detection from the person.
    
    Returns:
        True if handshake detected, False if timeout
    """
    start_time = time.perf_counter()
    detection_start_time = None
    last_status_update = 0
    
    log_say("Waiting for person to extend their hand for handshake...", True)
    
    while time.perf_counter() - start_time < timeout_s:
        try:
            observation = robot.get_observation()
                
            frame = observation[camera_name]
            
            # Process teleop commands during waiting (so user can position robot)
            if teleop is not None:
                action = teleop.get_action()
                robot.send_action(action)
            
            # Detect handshake gesture
            detection_result = handshake_detector.detect_handshake_gesture(frame, visualize=True)
            
            if detection_result['ready'] and detection_result['confidence'] >= confidence_threshold:
                if detection_start_time is None:
                    detection_start_time = time.perf_counter()
                    log_say(f"Handshake detected! Waiting {detection_delay} seconds before starting recording...", True)
                
                # Wait for the specified delay after detection
                if time.perf_counter() - detection_start_time >= detection_delay:
                    log_say("Starting handshake recording now!", True)
                    return True
            else:
                # Reset detection timer if gesture is lost
                detection_start_time = None
            
            # Always display clean data to Rerun during waiting phase (no OpenCV window)
            # Only show essential robot arm states (6 joint values) + camera with pose detection
            
            # Update status every second
            current_time = time.perf_counter()
            if current_time - last_status_update >= 1.0:
                status_text = f"WAITING | Episode: {episode_number} | Time remaining: {timeout_s - (current_time - start_time):.1f}s"
                rr.log("status", rr.TextLog(status_text, level=rr.TextLogLevel.INFO))
                last_status_update = current_time
            
            # Robot joint positions (6 values only) - grouped in single chart
            annotated_frame = detection_result.get('annotated_frame')
            robot_joints = {}
            for obs, val in observation.items():
                if isinstance(val, float) and obs.endswith('.pos'):
                    # Collect all robot joint positions for single chart
                    robot_joints[obs] = val
                elif isinstance(val, np.ndarray):
                    if obs == camera_name and annotated_frame is not None:
                        # Camera with pose detection - consistent path
                        rr.log("camera_with_pose", rr.Image(annotated_frame), static=True)
                        # Raw camera - consistent path
                        rr.log("camera_raw", rr.Image(val), static=True)
                    else:
                        # Raw camera for other cameras
                        rr.log("camera_raw", rr.Image(val), static=True)
            
            # Log all robot joints as single chart
            if robot_joints:
                # Log each joint individually to create clean grouped chart
                for joint_name, joint_val in robot_joints.items():
                    rr.log(f"robot_joints/{joint_name}", rr.Scalar(joint_val))
            
            time.sleep(0.1)  # Small delay to prevent excessive CPU usage
            
        except KeyboardInterrupt:
            logging.info("KeyboardInterrupt received in handshake detection")
            break
        except Exception as e:
            logging.error(f"Error in handshake detection loop: {e}")
            time.sleep(0.1)
            continue
    
    log_say("Handshake detection timeout. Skipping this episode.", True)
    return False


@safe_stop_image_writer
def record_handshake_loop(
    robot: Robot,
    events: dict,
    fps: int,
    handshake_detector: ImprovedHandshakeDetector,
    main_camera_name: str,
    dataset: LeRobotDataset | None = None,
    teleop: Teleoperator | None = None,
    policy: PreTrainedPolicy | None = None,
    control_time_s: int | None = None,
    single_task: str | None = None,
    display_data: bool = False,
    episode_number: int = 0,
    handshake_detection_fps: int = 10,
):
    if dataset is not None and dataset.fps != fps:
        raise ValueError(f"The dataset fps should be equal to requested fps ({dataset.fps} != {fps}).")

    # if policy is given it needs cleaning up
    if policy is not None:
        policy.reset()

    timestamp = 0
    start_episode_t = time.perf_counter()
    last_status_update = 0
    frame_count = 0
    
    # Handshake detection optimization - run at lower frequency
    detection_interval = max(1, fps // handshake_detection_fps)  # Run detection every N frames
    last_handshake_result = None
    
    while timestamp < control_time_s:
        start_loop_t = time.perf_counter()

        if events["exit_early"]:
            events["exit_early"] = False
            break

        observation = robot.get_observation()

        # Run handshake detection at reduced frequency for better performance
        if main_camera_name in observation:
            # Only run detection every N frames to improve FPS
            if frame_count % detection_interval == 0:
                frame = observation[main_camera_name]
                last_handshake_result = handshake_detector.detect_handshake_gesture(frame, visualize=True)
            
            # Use cached result (either fresh or from previous frame)
            if last_handshake_result is not None:
                # Add handshake detection data to observation for dataset recording
                handshake_ready = float(last_handshake_result['ready'])
                handshake_confidence = last_handshake_result['confidence']
                if last_handshake_result['hand_position'] is not None:
                    hand_position_x = float(last_handshake_result['hand_position'][0])
                    hand_position_y = float(last_handshake_result['hand_position'][1])
                else:
                    hand_position_x = -1.0
                    hand_position_y = -1.0
                
                # Add individual handshake values to observation (required by build_dataset_frame)
                observation["handshake_ready"] = handshake_ready
                observation["handshake_confidence"] = handshake_confidence
                observation["hand_position_x"] = hand_position_x
                observation["hand_position_y"] = hand_position_y
            else:
                # Fallback values if no detection result yet
                observation["handshake_ready"] = 0.0
                observation["handshake_confidence"] = 0.0
                observation["hand_position_x"] = -1.0
                observation["hand_position_y"] = -1.0

        if policy is not None or dataset is not None:
            observation_frame = build_dataset_frame(dataset.features, observation, prefix="observation")

        if policy is not None:
            action_values = predict_action(
                observation_frame,
                policy,
                get_safe_torch_device(policy.config.device),
                policy.config.use_amp,
                task=single_task,
                robot_type=robot.robot_type,
            )
            action = {key: action_values[i].item() for i, key in enumerate(robot.action_features)}
        elif policy is None and teleop is not None:
            action = teleop.get_action()
        else:
            logging.info(
                "No policy or teleoperator provided, skipping action generation."
                "This is likely to happen when resetting the environment without a teleop device."
                "The robot won't be at its rest position at the start of the next episode."
            )
            continue

        # Action can eventually be clipped using `max_relative_target`,
        # so action actually sent is saved in the dataset.
        sent_action = robot.send_action(action)

        if dataset is not None:
            action_frame = build_dataset_frame(dataset.features, sent_action, prefix="action")
            frame = {**observation_frame, **action_frame}
            dataset.add_frame(frame, task=single_task)

        if display_data:
            # Get pose overlay for camera feed using cached result
            annotated_frame = None
            if main_camera_name in observation and last_handshake_result and 'annotated_frame' in last_handshake_result:
                annotated_frame = last_handshake_result['annotated_frame']
            
            # Update status every second 
            current_time = time.perf_counter()
            if current_time - last_status_update >= 1.0:
                # Calculate actual FPS
                actual_fps = frame_count / timestamp if timestamp > 0 else 0
                status_text = f"RECORDING | Episode: {episode_number} | Elapsed: {timestamp:.1f}s | Remaining: {max(0, control_time_s - timestamp):.1f}s | Actual FPS: {actual_fps:.1f}"
                rr.log("status", rr.TextLog(status_text, level=rr.TextLogLevel.INFO))
                last_status_update = current_time
            
            # Robot joint positions (6 values only) - grouped in single chart
            robot_joints = {}
            for obs, val in observation.items():
                if isinstance(val, float) and obs.endswith('.pos'):
                    # Collect all robot joint positions for single chart
                    robot_joints[obs] = val
                elif isinstance(val, np.ndarray):
                    if obs == main_camera_name and annotated_frame is not None:
                        # Camera with pose detection - consistent path
                        rr.log("camera_with_pose", rr.Image(annotated_frame), static=True)
                        # Raw camera - consistent path
                        rr.log("camera_raw", rr.Image(val), static=True)
                    else:
                        # Raw camera for other cameras
                        rr.log("camera_raw", rr.Image(val), static=True)
            
            # Log all robot joints as single chart
            if robot_joints:
                # Log each joint individually to create clean grouped chart
                for joint_name, joint_val in robot_joints.items():
                    rr.log(f"robot_joints/{joint_name}", rr.Scalar(joint_val))
            
            # Actions are sent to robot but NOT displayed in charts to keep it clean (6 values only)

        dt_s = time.perf_counter() - start_loop_t
        busy_wait(1 / fps - dt_s)

        timestamp = time.perf_counter() - start_episode_t
        frame_count += 1


@parser.wrap()
def record_handshake(cfg: HandshakeRecordConfig) -> LeRobotDataset:
    init_logging()
    logging.info(pformat(asdict(cfg)))
    
    # Always initialize Rerun for live monitoring (robot states + pose detection)
    _init_rerun(session_name="handshake_recording")

    robot = make_robot_from_config(cfg.robot)
    teleop = make_teleoperator_from_config(cfg.teleop) if cfg.teleop is not None else None

    # Initialize handshake detector
    try:
        handshake_detector = ImprovedHandshakeDetector(
            confidence_threshold=cfg.dataset.handshake_confidence_threshold
        )
        logging.info(f"Handshake detector initialized with confidence threshold: {cfg.dataset.handshake_confidence_threshold}")
    except ImportError as e:
        logging.error(f"Failed to initialize handshake detector: {e}")
        logging.error("Please install required dependencies: pip install mediapipe opencv-python")
        raise

    # Determine main camera name (assume first camera is the main one)
    if not hasattr(robot.config, 'cameras') or not robot.config.cameras:
        raise ValueError("Robot must have at least one camera configured for handshake detection")
    
    main_camera_name = list(robot.config.cameras.keys())[0]
    logging.info(f"Using camera '{main_camera_name}' for handshake detection")

    # Build dataset features (without handshake features to avoid build_dataset_frame issues)
    action_features = hw_to_dataset_features(robot.action_features, "action", cfg.dataset.video)
    obs_features = hw_to_dataset_features(robot.observation_features, "observation", cfg.dataset.video)
    
    # Add handshake detection features as a single grouped feature (like robot joints)
    handshake_features = {
        "observation.handshake": {
            "dtype": "float32",
            "shape": (4,),
            "names": ["handshake_ready", "handshake_confidence", "hand_position_x", "hand_position_y"],
        },
    }
    
    dataset_features = {**action_features, **obs_features, **handshake_features}
    

    if cfg.resume:
        dataset = LeRobotDataset(
            cfg.dataset.repo_id,
            root=cfg.dataset.root,
        )

        if hasattr(robot, "cameras") and len(robot.cameras) > 0:
            dataset.start_image_writer(
                num_processes=cfg.dataset.num_image_writer_processes,
                num_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
            )
        sanity_check_dataset_robot_compatibility(dataset, robot, cfg.dataset.fps, dataset_features)
    else:
        # Create empty dataset or load existing saved episodes
        sanity_check_dataset_name(cfg.dataset.repo_id, cfg.policy)
        dataset = LeRobotDataset.create(
            cfg.dataset.repo_id,
            cfg.dataset.fps,
            root=cfg.dataset.root,
            robot_type=robot.name,
            features=dataset_features,
            use_videos=cfg.dataset.video,
            image_writer_processes=cfg.dataset.num_image_writer_processes,
            image_writer_threads=cfg.dataset.num_image_writer_threads_per_camera * len(robot.cameras),
        )

    # Load pretrained policy
    policy = None if cfg.policy is None else make_policy(cfg.policy, ds_meta=dataset.meta)

    robot.connect()
    if teleop is not None:
        teleop.connect()

    # Status will be shown as text overlay - no separate charts needed

    listener, events = init_keyboard_listener()

    recorded_episodes = 0
    while recorded_episodes < cfg.dataset.num_episodes and not events["stop_recording"]:
        log_say(f"Preparing to record handshake episode {dataset.num_episodes}", cfg.play_sounds)
        
        # Wait for handshake detection before starting episode
        handshake_detected = wait_for_handshake_detection(
            robot=robot,
            handshake_detector=handshake_detector,
            camera_name=main_camera_name,
            timeout_s=cfg.dataset.handshake_timeout_s,
            confidence_threshold=cfg.dataset.handshake_confidence_threshold,
            detection_delay=cfg.dataset.handshake_detection_delay,
            display_data=cfg.display_data,
            teleop=teleop,  # Enable teleop during waiting phase
            episode_number=dataset.num_episodes,
        )
        
        if not handshake_detected:
            log_say("Skipping episode due to handshake detection timeout", cfg.play_sounds)
            continue
        
        log_say(f"Recording handshake episode {dataset.num_episodes}", cfg.play_sounds)
        record_handshake_loop(
            robot=robot,
            events=events,
            fps=cfg.dataset.fps,
            handshake_detector=handshake_detector,
            main_camera_name=main_camera_name,
            teleop=teleop,
            policy=policy,
            dataset=dataset,
            control_time_s=cfg.dataset.episode_time_s,
            single_task=cfg.dataset.single_task,
            display_data=cfg.display_data,
            episode_number=dataset.num_episodes,
            handshake_detection_fps=cfg.dataset.handshake_detection_fps,
        )

        # Execute a few seconds without recording to give time to manually reset the environment
        # Skip reset for the last episode to be recorded
        if not events["stop_recording"] and (
            (recorded_episodes < cfg.dataset.num_episodes - 1) or events["rerecord_episode"]
        ):
            log_say("Reset the environment for next handshake", cfg.play_sounds)
            
            # Reset phase - status will be visible in console logs
            
            # Use regular record loop for reset (without handshake detection and without display)
            from lerobot.record import record_loop
            record_loop(
                robot=robot,
                events=events,
                fps=cfg.dataset.fps,
                teleop=teleop,
                control_time_s=cfg.dataset.reset_time_s,
                single_task=cfg.dataset.single_task,
                display_data=False,  # Disable display during reset to avoid extra windows
            )

        if events["rerecord_episode"]:
            log_say("Re-record episode", cfg.play_sounds)
            events["rerecord_episode"] = False
            events["exit_early"] = False
            dataset.clear_episode_buffer()
            continue

        dataset.save_episode()
        recorded_episodes += 1

    log_say("Stop recording handshake dataset", cfg.play_sounds, blocking=True)

    robot.disconnect()
    if teleop is not None:
        teleop.disconnect()

    if not is_headless() and listener is not None:
        listener.stop()

    # Close any OpenCV resources (cleanup)
    cv2.destroyAllWindows()

    log_say("Exiting", cfg.play_sounds)
    return dataset


if __name__ == "__main__":
    record_handshake()












